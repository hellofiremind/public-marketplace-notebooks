{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Firemind's FCA Compliance Model 8B Instruct Model Package from AWS Marketplace \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "AI-powered compliance assistant trained on the complete FCA Handbook. Provides expert guidance on UK financial regulations, compliance requirements, and regulatory changes for financial services firms seeking or maintaining FCA authorization.\n",
    "\n",
    "This sample notebook shows you how to deploy [Firemind's FCA Compliance Model](https://aws.amazon.com/marketplace/pp/prodview-x0idzsg6qgb0fctuzhm5qwe7) using Amazon SageMaker.\n",
    "\n",
    "> **Note**: This is a reference notebook and it cannot run unless you make changes suggested in the notebook.\n",
    "\n",
    "## Pre-requisites:\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. To deploy this ML model successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "    2. or your AWS account has a subscription to [Firemind's FCA Compliance Model](https://aws.amazon.com/marketplace/pp/prodview-x0idzsg6qgb0fctuzhm5qwe7). If so, skip step: [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "\n",
    "## Contents:\n",
    "1. [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "2. [Create an endpoint and perform real-time inference](#2.-Create-an-endpoint-and-perform-real-time-inference)\n",
    "   1. [Create an endpoint](#A.-Create-an-endpoint)\n",
    "   2. [Create input payload](#B.-Create-input-payload)\n",
    "   3. [Perform real-time inference](#C.-Perform-real-time-inference)\n",
    "   4. [Visualize output](#D.-Visualize-output)\n",
    "   5. [Delete the endpoint](#E.-Delete-the-endpoint)\n",
    "3. [Perform batch inference](#3.-Perform-batch-inference) \n",
    "4. [Clean-up](#4.-Clean-up)\n",
    "    1. [Delete the model](#A.-Delete-the-model)\n",
    "    2. [Unsubscribe to the listing (optional)](#B.-Unsubscribe-to-the-listing-(optional))\n",
    "    \n",
    "\n",
    "## Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Subscribe to the model package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To subscribe to the model package:\n",
    "1. Open the model package listing page [Firemind's FCA Compliance Model](https://aws.amazon.com/marketplace/pp/prodview-x0idzsg6qgb0fctuzhm5qwe7)\n",
    "1. On the AWS Marketplace listing, click on the **Continue to subscribe** button.\n",
    "1. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you and your organization agrees with EULA, pricing, and support terms. \n",
    "1. Once you click on **Continue to configuration button** and then choose a **region**, you will see a **Product Arn** displayed. This is the model package ARN that you need to specify while creating a deployable model using Boto3. Copy the ARN corresponding to your region and specify the same in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION SECTION ===\n",
    "# Replace these values with your actual AWS resources\n",
    "\n",
    "# 1. Model Package ARN from AWS Marketplace subscription\n",
    "# Get this from: https://aws.amazon.com/marketplace/pp/prodview-x0idzsg6qgb0fctuzhm5qwe7\n",
    "model_package_arn = \"<Customer to specify Model package ARN corresponding to their AWS region>\"\n",
    "\n",
    "# 2. SageMaker Execution Role ARN\n",
    "# Create or use existing role with SageMaker permissions\n",
    "role_arn = \"arn:aws:iam::YOUR_ACCOUNT_ID:role/SageMakerExecutionRole\"\n",
    "\n",
    "# 3. Model Artifact S3 URI (will be provided by the model package)\n",
    "model_data_s3_uri = \"s3://YOUR_BUCKET/path/to/model.tar.gz\"\n",
    "\n",
    "# 4. Container Image URI (for hosting the model)\n",
    "container_image = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.2.0-gpu-py39-cu118-ubuntu20.04\"\n",
    "\n",
    "# 5. Deployment Configuration\n",
    "model_name = \"fca-compliance-model\"\n",
    "endpoint_config_name = f\"{model_name}-cfg\"\n",
    "endpoint_name = f\"{model_name}-ep\"\n",
    "instance_type = \"ml.g5.2xlarge\"  # Adjust based on your needs and budget\n",
    "instance_count = 1\n",
    "\n",
    "print(\"Configuration loaded. Please update the placeholder values above before proceeding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from datetime import datetime\n",
    "import io\n",
    "print('boto3 version:', boto3.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === AWS CONFIGURATION ===\n",
    "profile_name = os.environ.get('AWS_PROFILE', 'default')\n",
    "region_name = os.environ.get('AWS_REGION', 'us-east-1')\n",
    "\n",
    "# Create a Boto3 session using the specified profile\n",
    "boto3_session = boto3.Session(profile_name=profile_name, region_name=region_name)\n",
    "\n",
    "# SageMaker runtime client (for invoking endpoints)\n",
    "sagemaker_runtime = boto3_session.client('sagemaker-runtime', region_name=region_name)\n",
    "\n",
    "# SageMaker management client (for creating models/endpoints)\n",
    "sagemaker = boto3_session.client('sagemaker', region_name=region_name)\n",
    "\n",
    "# Marketplace client for model package operations\n",
    "marketplace = boto3_session.client('marketplace-catalog', region_name=region_name)\n",
    "\n",
    "print('Using profile:', profile_name, 'region:', region_name)\n",
    "\n",
    "# === CONFIGURATION VALIDATION ===\n",
    "def validate_configuration():\n",
    "    \"\"\"Validate that all required configuration values are set.\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    if model_package_arn == \"<Customer to specify Model package ARN corresponding to their AWS region>\":\n",
    "        issues.append(\"‚ùå model_package_arn is not set\")\n",
    "    \n",
    "    if role_arn == \"arn:aws:iam::YOUR_ACCOUNT_ID:role/SageMakerExecutionRole\":\n",
    "        issues.append(\"‚ùå role_arn is not set\")\n",
    "    \n",
    "    if model_data_s3_uri == \"s3://YOUR_BUCKET/path/to/model.tar.gz\":\n",
    "        issues.append(\"‚ùå model_data_s3_uri is not set\")\n",
    "    \n",
    "    if container_image == \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.2.0-gpu-py39-cu118-ubuntu20.04\":\n",
    "        print(\"‚ö†Ô∏è  Using default container image. Verify this is correct for your model.\")\n",
    "    \n",
    "    if issues:\n",
    "        print(\"üö® Configuration Issues Found:\")\n",
    "        for issue in issues:\n",
    "            print(f\"  {issue}\")\n",
    "        print(\"\\nPlease update the configuration values before proceeding.\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"‚úÖ Configuration validation passed!\")\n",
    "        return True\n",
    "\n",
    "# === SECURITY & PERMISSION VALIDATION ===\n",
    "def validate_aws_permissions():\n",
    "    \"\"\"Validate that the required AWS permissions are available.\"\"\"\n",
    "    required_permissions = [\n",
    "        'sagemaker:CreateModel',\n",
    "        'sagemaker:CreateEndpoint',\n",
    "        'sagemaker:CreateEndpointConfig',\n",
    "        'sagemaker:InvokeEndpoint',\n",
    "        'sagemaker:DescribeModel',\n",
    "        'sagemaker:DescribeEndpoint',\n",
    "        'sagemaker:DescribeEndpointConfig',\n",
    "        'sagemaker:CreateTransformJob',\n",
    "        'sagemaker:DescribeTransformJob',\n",
    "        'iam:PassRole'\n",
    "    ]\n",
    "    \n",
    "    print(\"üîê Validating AWS permissions...\")\n",
    "    \n",
    "    # Test basic SageMaker access\n",
    "    try:\n",
    "        sagemaker.list_models(MaxResults=1)\n",
    "        print(\"‚úÖ SageMaker access confirmed\")\n",
    "    except ClientError as e:\n",
    "        print(f\"‚ùå SageMaker access failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test IAM role access\n",
    "    try:\n",
    "        iam = boto3_session.client('iam')\n",
    "        iam.get_role(RoleName=role_arn.split('/')[-1])\n",
    "        print(\"‚úÖ IAM role access confirmed\")\n",
    "    except ClientError as e:\n",
    "        print(f\"‚ùå IAM role access failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"‚úÖ Permission validation completed\")\n",
    "    return True\n",
    "\n",
    "def validate_model_package_access():\n",
    "    \"\"\"Validate access to the model package.\"\"\"\n",
    "    if model_package_arn == \"<Customer to specify Model package ARN corresponding to their AWS region>\":\n",
    "        print(\"‚ö†Ô∏è  Model package ARN not configured\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        model_package = get_model_package_details(model_package_arn)\n",
    "        print(f\"‚úÖ Model package access confirmed: {model_package['ModelPackageName']}\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        print(f\"‚ùå Model package access failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run all validations\n",
    "print(\"üîç Running comprehensive validation...\")\n",
    "config_valid = validate_configuration()\n",
    "perms_valid = validate_aws_permissions()\n",
    "package_valid = validate_model_package_access()\n",
    "\n",
    "if config_valid and perms_valid and package_valid:\n",
    "    print(\"\\n‚úÖ All validations passed! Ready to proceed with deployment.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Some validations failed. Please address the issues above before proceeding.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create an endpoint and perform real-time inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODEL PACKAGE INTEGRATION ===\n",
    "def get_model_package_details(model_package_arn):\n",
    "    \"\"\"Get model package details from SageMaker.\"\"\"\n",
    "    try:\n",
    "        response = sagemaker.describe_model_package(ModelPackageName=model_package_arn)\n",
    "        return response\n",
    "    except ClientError as e:\n",
    "        print(f\"Error getting model package details: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_model_from_package(model_package_arn, model_name, role_arn):\n",
    "    \"\"\"Create a SageMaker model from a model package.\"\"\"\n",
    "    try:\n",
    "        # Get model package details\n",
    "        model_package = get_model_package_details(model_package_arn)\n",
    "        \n",
    "        # Create model from package\n",
    "        response = sagemaker.create_model(\n",
    "            ModelName=model_name,\n",
    "            PrimaryContainer={\n",
    "                'Image': model_package['InferenceSpecification']['Containers'][0]['Image'],\n",
    "                'ModelDataUrl': model_package['InferenceSpecification']['Containers'][0]['ModelDataUrl'],\n",
    "                'Environment': model_package['InferenceSpecification']['Containers'][0].get('Environment', {})\n",
    "            },\n",
    "            ExecutionRoleArn=role_arn\n",
    "        )\n",
    "        print(f\"‚úÖ Model {model_name} created successfully from package\")\n",
    "        return response\n",
    "    except ClientError as e:\n",
    "        print(f\"Error creating model from package: {e}\")\n",
    "        raise\n",
    "\n",
    "# === COST ESTIMATION ===\n",
    "def estimate_endpoint_cost(instance_type, instance_count, hours=1):\n",
    "    \"\"\"Estimate cost for running endpoint (approximate).\"\"\"\n",
    "    # Approximate hourly costs for common instance types\n",
    "    costs = {\n",
    "        'ml.g5.2xlarge': 1.21,\n",
    "        'ml.g5.4xlarge': 2.42,\n",
    "        'ml.g5.8xlarge': 4.84,\n",
    "        'ml.g5.12xlarge': 7.26,\n",
    "        'ml.g5.16xlarge': 9.68,\n",
    "        'ml.g5.24xlarge': 14.52,\n",
    "        'ml.g5.48xlarge': 29.04\n",
    "    }\n",
    "    \n",
    "    hourly_cost = costs.get(instance_type, 0) * instance_count\n",
    "    total_cost = hourly_cost * hours\n",
    "    \n",
    "    print(f\"üí∞ Estimated cost for {instance_type} x{instance_count} for {hours} hour(s): ${total_cost:.2f}\")\n",
    "    return total_cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section provides utility functions and example usage to:\n",
    "- register a SageMaker model (CreateModel)\n",
    "- create an EndpointConfig (CreateEndpointConfig)\n",
    "- create an Endpoint (CreateEndpoint)\n",
    "\n",
    "You need the following pieces of information to deploy:\n",
    "- role_arn: IAM execution role for SageMaker (must allow SageMaker to pull from S3 and create network interfaces)\n",
    "- model_data_s3_uri: S3 URI of the model artifact (tar.gz) containing model weights/config\n",
    "- container_image: the container image URI to use for hosting (ECR image that knows how to serve your model). For TGI/DJC models hosted via SageMaker, use the appropriate container image.\n",
    "- instance_type and instance_count for the endpoint\n",
    "\n",
    "The functions below are defensive (check for existing resources and optionally update)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists_model(model_name):\n",
    "    try:\n",
    "        sagemaker.describe_model(ModelName=model_name)\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'ValidationException':\n",
    "            return False\n",
    "        raise\n",
    "\n",
    "def exists_endpoint_config(cfg_name):\n",
    "    try:\n",
    "        sagemaker.describe_endpoint_config(EndpointConfigName=cfg_name)\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'ValidationException':\n",
    "            return False\n",
    "        raise\n",
    "\n",
    "def exists_endpoint(endpoint_name):\n",
    "    try:\n",
    "        sagemaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'ValidationException':\n",
    "            return False\n",
    "        raise\n",
    "\n",
    "def wait_for_endpoint(endpoint_name, poll_interval=10, timeout_minutes=30):\n",
    "    \"\"\"Poll endpoint status until InService or Failed. Returns final status dict.\"\"\"\n",
    "    start = datetime.utcnow()\n",
    "    timeout = timeout_minutes * 60\n",
    "    while True:\n",
    "        resp = sagemaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "        status = resp['EndpointStatus']\n",
    "        print(f'[{datetime.utcnow().isoformat()}] Endpoint {endpoint_name} status: {status}')\n",
    "        if status in ('InService', 'Failed'):\n",
    "            return resp\n",
    "        if (datetime.utcnow() - start).total_seconds() > timeout:\n",
    "            raise TimeoutError(f'Endpoint {endpoint_name} did not become InService within timeout')\n",
    "        time.sleep(poll_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Create an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_deploy_model_endpoint(\n",
    "    model_name,\n",
    "    model_data_s3_uri,\n",
    "    container_image,\n",
    "    role_arn,\n",
    "    endpoint_config_name=None,\n",
    "    endpoint_name=None,\n",
    "    instance_type='ml.g5.2xlarge',\n",
    "    instance_count=1,\n",
    "    wait=True,\n",
    "    update_if_exists=False\n",
    "):\n",
    "    \"\"\"Create a SageMaker model, endpoint config and endpoint.\n",
    "    If update_if_exists==True and endpoint exists, this will create a new endpoint config and call UpdateEndpoint.\n",
    "    Returns the final endpoint description.\n",
    "    \"\"\"\n",
    "    endpoint_config_name = endpoint_config_name or f'{model_name}-cfg-{int(time.time())}'\n",
    "    endpoint_name = endpoint_name or f'{model_name}-ep-{int(time.time())}'\n",
    "\n",
    "    # 1) Create Model\n",
    "    container_def = {\n",
    "        'Image': container_image,\n",
    "        'ModelDataUrl': model_data_s3_uri,\n",
    "        # Optionally pass env vars to container\n",
    "        'Environment': {}\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        if exists_model(model_name):\n",
    "            if update_if_exists:\n",
    "                print(f'Model {model_name} already exists; will re-register model by deleting and recreating.')\n",
    "                # Deleting model is optional and must be done carefully if endpoints are still using it.\n",
    "                try:\n",
    "                    sagemaker.delete_model(ModelName=model_name)\n",
    "                except Exception as e:\n",
    "                    print('Warning: could not delete existing model:', e)\n",
    "            else:\n",
    "                print(f'Model {model_name} already exists. Skipping CreateModel.')\n",
    "        print('Creating model:', model_name)\n",
    "        sagemaker.create_model(\n",
    "            ModelName=model_name,\n",
    "            PrimaryContainer=container_def,\n",
    "            ExecutionRoleArn=role_arn\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        raise\n",
    "\n",
    "    # 2) Create Endpoint Config\n",
    "    production_variants = [\n",
    "        {\n",
    "            'VariantName': 'AllTraffic',\n",
    "            'ModelName': model_name,\n",
    "            'InitialInstanceCount': instance_count,\n",
    "            'InstanceType': instance_type,\n",
    "            'InitialVariantWeight': 1.0\n",
    "        }\n",
    "    ]\n",
    "    try:\n",
    "        if exists_endpoint_config(endpoint_config_name):\n",
    "            if update_if_exists:\n",
    "                print(f'EndpointConfig {endpoint_config_name} already exists; deleting and recreating.')\n",
    "                sagemaker.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "            else:\n",
    "                print(f'EndpointConfig {endpoint_config_name} already exists. Skipping CreateEndpointConfig.')\n",
    "        print('Creating endpoint config:', endpoint_config_name)\n",
    "        sagemaker.create_endpoint_config(\n",
    "            EndpointConfigName=endpoint_config_name,\n",
    "            ProductionVariants=production_variants\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        raise\n",
    "\n",
    "    # 3) Create or Update Endpoint\n",
    "    try:\n",
    "        if exists_endpoint(endpoint_name):\n",
    "            if update_if_exists:\n",
    "                print(f'Endpoint {endpoint_name} already exists; updating to new config {endpoint_config_name}')\n",
    "                sagemaker.update_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n",
    "            else:\n",
    "                print(f'Endpoint {endpoint_name} already exists. Skipping CreateEndpoint.')\n",
    "                \n",
    "        else:\n",
    "            print('Creating endpoint:', endpoint_name)\n",
    "            sagemaker.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n",
    "\n",
    "        if wait:\n",
    "            desc = wait_for_endpoint(endpoint_name)\n",
    "            return desc\n",
    "        else:\n",
    "            return sagemaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "    except ClientError as e:\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once endpoint has been created, you would be able to perform real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Create input payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEPLOY MODEL FROM PACKAGE ===\n",
    "# This section creates and deploys the model using the model package\n",
    "\n",
    "# Validate configuration before proceeding\n",
    "if not validate_configuration():\n",
    "    print(\"‚ùå Please fix configuration issues before proceeding.\")\n",
    "else:\n",
    "    # Show cost estimation\n",
    "    estimate_endpoint_cost(instance_type, instance_count, hours=1)\n",
    "    \n",
    "    print(f\"\\nüöÄ Starting deployment of {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Option 1: Use model package (recommended)\n",
    "        if model_package_arn != \"<Customer to specify Model package ARN corresponding to their AWS region>\":\n",
    "            print(\"üì¶ Creating model from package...\")\n",
    "            create_model_from_package(model_package_arn, model_name, role_arn)\n",
    "            \n",
    "            # Get model package details for container info\n",
    "            model_package = get_model_package_details(model_package_arn)\n",
    "            container_image = model_package['InferenceSpecification']['Containers'][0]['Image']\n",
    "            model_data_s3_uri = model_package['InferenceSpecification']['Containers'][0]['ModelDataUrl']\n",
    "            \n",
    "            print(f\"‚úÖ Using container: {container_image}\")\n",
    "            print(f\"‚úÖ Using model data: {model_data_s3_uri}\")\n",
    "        \n",
    "        # Create endpoint configuration and deploy\n",
    "        desc = create_and_deploy_model_endpoint(\n",
    "            model_name=model_name,\n",
    "            model_data_s3_uri=model_data_s3_uri,\n",
    "            container_image=container_image,\n",
    "            role_arn=role_arn,\n",
    "            endpoint_config_name=endpoint_config_name,\n",
    "            endpoint_name=endpoint_name,\n",
    "            instance_type=instance_type,\n",
    "            instance_count=instance_count,\n",
    "            wait=True,\n",
    "            update_if_exists=True\n",
    "        )\n",
    "        \n",
    "        print('‚úÖ Endpoint deployment completed!')\n",
    "        print('Final endpoint description:')\n",
    "        print(json.dumps(desc, indent=2, default=str))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Deployment failed: {e}\")\n",
    "        print(\"Please check your configuration and try again.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the endpoint is InService, you can invoke it through the runtime client. Adjust the payload format to what the hosting container expects. Examples earlier in this notebook show both the \"inputs\" style and the \"messages\" style used by different model server wrappers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_text_endpoint(endpoint_name, prompt, timeout_seconds=120, max_retries=3):\n",
    "    \"\"\"Enhanced endpoint invocation with retry logic and better error handling.\"\"\"\n",
    "    payload = {\n",
    "        'inputs': prompt,\n",
    "        'parameters': {\n",
    "            'do_sample': True,\n",
    "            'max_new_tokens': 256,\n",
    "            'temperature': 0.2\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = sagemaker_runtime.invoke_endpoint(\n",
    "                EndpointName=endpoint_name,\n",
    "                ContentType='application/json',\n",
    "                Body=json.dumps(payload)\n",
    "            )\n",
    "            out = response['Body'].read().decode('utf-8')\n",
    "            return json.loads(out)\n",
    "        except ClientError as e:\n",
    "            error_code = e.response['Error']['Code']\n",
    "            if error_code == 'ThrottlingException' and attempt < max_retries - 1:\n",
    "                wait_time = 2 ** attempt  # Exponential backoff\n",
    "                print(f\"‚è≥ Throttling detected, retrying in {wait_time} seconds... (attempt {attempt + 1}/{max_retries})\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            else:\n",
    "                print(f'‚ùå Invoke failed after {attempt + 1} attempts: {e}')\n",
    "                raise\n",
    "        except Exception as e:\n",
    "            print(f'‚ùå Unexpected error: {e}')\n",
    "            raise\n",
    "\n",
    "def safe_invoke_endpoint(endpoint_name, prompt, **kwargs):\n",
    "    \"\"\"Safely invoke endpoint with comprehensive error handling.\"\"\"\n",
    "    try:\n",
    "        # Check if endpoint exists and is in service\n",
    "        if not exists_endpoint(endpoint_name):\n",
    "            raise ValueError(f\"Endpoint {endpoint_name} does not exist\")\n",
    "        \n",
    "        endpoint_status = sagemaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "        if endpoint_status['EndpointStatus'] != 'InService':\n",
    "            raise ValueError(f\"Endpoint {endpoint_name} is not in service. Status: {endpoint_status['EndpointStatus']}\")\n",
    "        \n",
    "        # Invoke endpoint\n",
    "        result = invoke_text_endpoint(endpoint_name, prompt, **kwargs)\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error invoking endpoint: {e}\")\n",
    "        return None\n",
    "\n",
    "# === EXAMPLE USAGE ===\n",
    "def test_endpoint_with_sample_queries():\n",
    "    \"\"\"Test the endpoint with sample FCA compliance queries.\"\"\"\n",
    "    sample_queries = [\n",
    "        \"What are the key FCA compliance requirements for financial services firms?\",\n",
    "        \"Explain the regulatory framework for UK banks under FCA supervision.\",\n",
    "        \"What are the capital adequacy requirements for FCA-authorized firms?\"\n",
    "    ]\n",
    "    \n",
    "    if exists_endpoint(endpoint_name):\n",
    "        print(f\"üß™ Testing endpoint {endpoint_name} with sample queries...\")\n",
    "        \n",
    "        for i, query in enumerate(sample_queries, 1):\n",
    "            print(f\"\\nüìù Query {i}: {query}\")\n",
    "            result = safe_invoke_endpoint(endpoint_name, query)\n",
    "            \n",
    "            if result:\n",
    "                print(f\"‚úÖ Response: {result}\")\n",
    "            else:\n",
    "                print(\"‚ùå Failed to get response\")\n",
    "    else:\n",
    "        print(f\"‚ùå Endpoint {endpoint_name} does not exist or is not ready\")\n",
    "\n",
    "print(\"üîß Enhanced endpoint invocation functions loaded.\")\n",
    "print(\"Use test_endpoint_with_sample_queries() to test your endpoint with sample FCA queries.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Delete the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a real-time inference, you do not need the endpoint any more. You can terminate the endpoint to avoid being charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_endpoint_resources(endpoint_name=None, endpoint_config_name=None, model_name=None, wait_for_deletion=True):\n",
    "    if endpoint_name and exists_endpoint(endpoint_name):\n",
    "        print('Deleting endpoint:', endpoint_name)\n",
    "        sagemaker.delete_endpoint(EndpointName=endpoint_name)\n",
    "        if wait_for_deletion:\n",
    "            # wait until endpoint no longer exists\n",
    "            while exists_endpoint(endpoint_name):\n",
    "                print('Waiting for endpoint deletion...')\n",
    "                time.sleep(5)\n",
    "    if endpoint_config_name and exists_endpoint_config(endpoint_config_name):\n",
    "        print('Deleting endpoint config:', endpoint_config_name)\n",
    "        sagemaker.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "    if model_name and exists_model(model_name):\n",
    "        print('Deleting model:', model_name)\n",
    "        try:\n",
    "            sagemaker.delete_model(ModelName=model_name)\n",
    "        except Exception as e:\n",
    "            print('Could not delete model:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perform batch inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BATCH INFERENCE IMPLEMENTATION ===\n",
    "import pandas as pd\n",
    "from sagemaker.s3 import S3Uploader, S3Downloader\n",
    "\n",
    "def create_batch_transform_job(\n",
    "    model_name,\n",
    "    input_s3_uri,\n",
    "    output_s3_uri,\n",
    "    instance_type='ml.g5.2xlarge',\n",
    "    instance_count=1,\n",
    "    max_payload_size=6,  # MB\n",
    "    job_name=None\n",
    "):\n",
    "    \"\"\"Create a SageMaker batch transform job.\"\"\"\n",
    "    job_name = job_name or f\"{model_name}-batch-{int(time.time())}\"\n",
    "    \n",
    "    try:\n",
    "        response = sagemaker.create_transform_job(\n",
    "            TransformJobName=job_name,\n",
    "            ModelName=model_name,\n",
    "            MaxPayloadInMB=max_payload_size,\n",
    "            BatchStrategy='MultiRecord',\n",
    "            TransformInput={\n",
    "                'DataSource': {\n",
    "                    'S3DataSource': {\n",
    "                        'S3DataType': 'S3Prefix',\n",
    "                        'S3Uri': input_s3_uri\n",
    "                    }\n",
    "                },\n",
    "                'ContentType': 'application/json',\n",
    "                'SplitType': 'Line'\n",
    "            },\n",
    "            TransformOutput={\n",
    "                'S3OutputPath': output_s3_uri,\n",
    "                'Accept': 'application/json'\n",
    "            },\n",
    "            TransformResources={\n",
    "                'InstanceType': instance_type,\n",
    "                'InstanceCount': instance_count\n",
    "            }\n",
    "        )\n",
    "        print(f\"‚úÖ Batch transform job '{job_name}' created successfully\")\n",
    "        return response\n",
    "    except ClientError as e:\n",
    "        print(f\"‚ùå Error creating batch transform job: {e}\")\n",
    "        raise\n",
    "\n",
    "def wait_for_batch_job_completion(job_name, poll_interval=30):\n",
    "    \"\"\"Wait for batch transform job to complete.\"\"\"\n",
    "    print(f\"‚è≥ Waiting for batch job '{job_name}' to complete...\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            response = sagemaker.describe_transform_job(TransformJobName=job_name)\n",
    "            status = response['TransformJobStatus']\n",
    "            print(f\"üìä Job status: {status}\")\n",
    "            \n",
    "            if status in ['Completed', 'Failed', 'Stopped']:\n",
    "                return response\n",
    "                \n",
    "            time.sleep(poll_interval)\n",
    "        except ClientError as e:\n",
    "            print(f\"‚ùå Error checking job status: {e}\")\n",
    "            raise\n",
    "\n",
    "def prepare_batch_input(input_data, s3_bucket, s3_prefix):\n",
    "    \"\"\"Prepare input data for batch inference.\"\"\"\n",
    "    # Convert input data to JSONL format\n",
    "    if isinstance(input_data, list):\n",
    "        jsonl_data = '\\n'.join([json.dumps(item) for item in input_data])\n",
    "    else:\n",
    "        jsonl_data = input_data\n",
    "    \n",
    "    # Upload to S3\n",
    "    s3_uri = f\"s3://{s3_bucket}/{s3_prefix}/input.jsonl\"\n",
    "    S3Uploader.upload_string(jsonl_data, s3_uri)\n",
    "    print(f\"‚úÖ Input data uploaded to: {s3_uri}\")\n",
    "    return s3_uri\n",
    "\n",
    "def process_batch_output(output_s3_uri):\n",
    "    \"\"\"Process and download batch inference results.\"\"\"\n",
    "    try:\n",
    "        # Download results\n",
    "        results = S3Downloader.download(output_s3_uri, './batch_output/')\n",
    "        print(f\"‚úÖ Batch results downloaded to: ./batch_output/\")\n",
    "        \n",
    "        # Process results\n",
    "        output_files = [f for f in os.listdir('./batch_output/') if f.endswith('.jsonl')]\n",
    "        all_results = []\n",
    "        \n",
    "        for file in output_files:\n",
    "            with open(f'./batch_output/{file}', 'r') as f:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        all_results.append(json.loads(line.strip()))\n",
    "        \n",
    "        return all_results\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing batch output: {e}\")\n",
    "        raise\n",
    "\n",
    "# === EXAMPLE BATCH INFERENCE ===\n",
    "def run_batch_inference_example():\n",
    "    \"\"\"Example of running batch inference.\"\"\"\n",
    "    # Sample input data\n",
    "    sample_inputs = [\n",
    "        {\"inputs\": \"What are the key FCA compliance requirements for financial services?\"},\n",
    "        {\"inputs\": \"Explain the regulatory framework for UK banks.\"},\n",
    "        {\"inputs\": \"What are the capital adequacy requirements under FCA rules?\"}\n",
    "    ]\n",
    "    \n",
    "    # Configuration\n",
    "    s3_bucket = \"your-batch-inference-bucket\"  # Replace with your bucket\n",
    "    s3_prefix = \"fca-batch-inference\"\n",
    "    input_s3_uri = prepare_batch_input(sample_inputs, s3_bucket, f\"{s3_prefix}/input\")\n",
    "    output_s3_uri = f\"s3://{s3_bucket}/{s3_prefix}/output\"\n",
    "    \n",
    "    # Create batch transform job\n",
    "    job_response = create_batch_transform_job(\n",
    "        model_name=model_name,\n",
    "        input_s3_uri=input_s3_uri,\n",
    "        output_s3_uri=output_s3_uri,\n",
    "        instance_type=instance_type,\n",
    "        instance_count=1\n",
    "    )\n",
    "    \n",
    "    # Wait for completion\n",
    "    final_response = wait_for_batch_job_completion(job_response['TransformJobName'])\n",
    "    \n",
    "    if final_response['TransformJobStatus'] == 'Completed':\n",
    "        print(\"‚úÖ Batch inference completed successfully!\")\n",
    "        results = process_batch_output(output_s3_uri)\n",
    "        return results\n",
    "    else:\n",
    "        print(f\"‚ùå Batch inference failed: {final_response.get('FailureReason', 'Unknown error')}\")\n",
    "        return None\n",
    "\n",
    "print(\"üìã Batch inference functions loaded. Use run_batch_inference_example() to run a sample batch job.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Troubleshooting Guide\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "\n",
    "#### 1. **Configuration Issues**\n",
    "- **Issue**: \"Configuration validation failed\"\n",
    "- **Solution**: Update all placeholder values in the configuration section\n",
    "- **Check**: Ensure `model_package_arn`, `role_arn`, and other values are properly set\n",
    "\n",
    "#### 2. **Permission Issues**\n",
    "- **Issue**: \"Access denied\" or \"Insufficient permissions\"\n",
    "- **Solution**: Ensure your IAM role has the required SageMaker permissions\n",
    "- **Required permissions**: `AmazonSageMakerFullAccess` or equivalent\n",
    "\n",
    "#### 3. **Model Package Issues**\n",
    "- **Issue**: \"Model package not found\" or \"Access denied to model package\"\n",
    "- **Solution**: Ensure you have subscribed to the FCA Compliance Model on AWS Marketplace\n",
    "- **Check**: Verify the model package ARN is correct for your region\n",
    "\n",
    "#### 4. **Endpoint Deployment Issues**\n",
    "- **Issue**: Endpoint creation fails or times out\n",
    "- **Solution**: Check instance type availability in your region\n",
    "- **Alternative**: Try a different instance type (e.g., `ml.g5.4xlarge` instead of `ml.g5.2xlarge`)\n",
    "\n",
    "#### 5. **Cost Management**\n",
    "- **Issue**: Unexpected charges\n",
    "- **Solution**: Always delete endpoints when not in use\n",
    "- **Tip**: Use the cleanup functions provided in this notebook\n",
    "\n",
    "#### 6. **Network Issues**\n",
    "- **Issue**: Endpoint not accessible or slow responses\n",
    "- **Solution**: Check VPC configuration and security groups\n",
    "- **Consider**: Using VPC endpoints for better security\n",
    "\n",
    "### üìû Support Resources:\n",
    "- [AWS SageMaker Documentation](https://docs.aws.amazon.com/sagemaker/)\n",
    "- [AWS Marketplace Support](https://aws.amazon.com/marketplace/help)\n",
    "- [FCA Compliance Model Support](https://aws.amazon.com/marketplace/pp/prodview-x0idzsg6qgb0fctuzhm5qwe7)\n",
    "\n",
    "### üí° Best Practices:\n",
    "- Always test with small workloads first\n",
    "- Monitor costs using AWS Cost Explorer\n",
    "- Use appropriate instance types for your workload\n",
    "- Implement proper error handling in production\n",
    "- Consider using serverless endpoints for variable workloads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Unsubscribe to the listing (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to unsubscribe to the model package, follow these steps. Before you cancel the subscription, ensure that you do not have any [deployable model](https://console.aws.amazon.com/sagemaker/home#/models) created from the model package or using the algorithm. Note - You can find this information by looking at the container name associated with the model. \n",
    "\n",
    "**Steps to unsubscribe to product from AWS Marketplace**:\n",
    "1. Navigate to __Machine Learning__ tab on [__Your Software subscriptions page__](https://aws.amazon.com/marketplace/ai/library?productType=ml&ref_=mlmp_gitdemo_indust)\n",
    "2. Locate the listing that you want to cancel the subscription for, and then choose __Cancel Subscription__  to cancel the subscription.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
